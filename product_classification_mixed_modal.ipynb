{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdamWatson91/fb_marketplace/blob/main/product_classification_mixed_modal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoJiSyiPA9z6",
        "outputId": "4a64915d-8fbd-4004-b92e-53880797bee5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jpLZMPyv4N5v"
      },
      "outputs": [],
      "source": [
        "!mkdir data\n",
        "!cp \"/content/drive/MyDrive/text_model_data.npy\" \"data/text_model_data.npy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqBcnBeBv_R3",
        "outputId": "ac6499eb-b3a0-4bee-98be-ba0d1a418240"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 31.1 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 62.8 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 10.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 71.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"
          ]
        }
      ],
      "source": [
        "# Install module for Bert model\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Module imports and dataset imports"
      ],
      "metadata": {
        "id": "ux_rXS4FtNU3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "7dIA53ML1eMN",
        "outputId": "7be02487-cee3-482d-a4b7-de43a840c8fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda for inference\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                 product_id  \\\n",
              "0      243809c0-9cfc-4486-ad12-3b7a16605ba9   \n",
              "1      1c58d3f9-8b93-47ea-9415-204fcc2a22e6   \n",
              "2      1c58d3f9-8b93-47ea-9415-204fcc2a22e6   \n",
              "3      860673f1-57f6-47ba-8d2f-13f9e05b8f9a   \n",
              "4      59948726-29be-4b35-ade5-bb2fd7331856   \n",
              "...                                     ...   \n",
              "11638  2b0a652b-46a2-4297-b619-5efeeb222787   \n",
              "11639  719fd40a-870e-4144-b324-55dff2e66fb4   \n",
              "11640  719fd40a-870e-4144-b324-55dff2e66fb4   \n",
              "11641  86d1806b-5575-4a7e-9160-f24f12be6c95   \n",
              "11642  86d1806b-5575-4a7e-9160-f24f12be6c95   \n",
              "\n",
              "                                     product_description  \\\n",
              "0      Mirror wall art. Posted by Nisha in Dining, Li...   \n",
              "1      Morphy Richard’s (model no 48755)Stainless ste...   \n",
              "2      Morphy Richard’s (model no 48755)Stainless ste...   \n",
              "3      I have 2 of these - collection only as I don’t...   \n",
              "4      Great reclaimed army ammunition box used as co...   \n",
              "...                                                  ...   \n",
              "11638  Pick up only £250Comes with two pistols stocks...   \n",
              "11639  Bought at christmas from currys retailing at £...   \n",
              "11640  Bought at christmas from currys retailing at £...   \n",
              "11641  Nintendo Switch console only used 2/3 times. A...   \n",
              "11642  Nintendo Switch console only used 2/3 times. A...   \n",
              "\n",
              "                      category                                img_id  \\\n",
              "0               Home & Garden   64aa79f3-e9fa-417c-a332-714b8ce933f1   \n",
              "1               Home & Garden   4e670f9e-7feb-458f-b529-ac52547abe2b   \n",
              "2               Home & Garden   a864ee52-d91e-46e7-94d1-2418e9bb2877   \n",
              "3               Home & Garden   bfe77c38-c9eb-47fb-b3d6-31ffdefb6ff9   \n",
              "4               Home & Garden   a92e56b7-94fc-41b4-ba6c-f2f224f42bb8   \n",
              "...                        ...                                   ...   \n",
              "11638  Video Games & Consoles   b1466df4-594b-4f83-a66c-f9835d982c92   \n",
              "11639  Video Games & Consoles   c8488028-bf07-4258-a4c2-56d2fe387835   \n",
              "11640  Video Games & Consoles   dc99e40f-6b15-494d-9fb7-f0d02e9781f9   \n",
              "11641  Video Games & Consoles   ed2aaf88-616c-4e6b-af49-bbdf7c0fbd06   \n",
              "11642  Video Games & Consoles   c6113145-89c8-47cd-9211-38f29d016cc7   \n",
              "\n",
              "                                               img_array  \n",
              "0      [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...  \n",
              "1      [[[255, 255, 255], [255, 255, 255], [255, 255,...  \n",
              "2      [[[255, 255, 255], [255, 255, 255], [255, 255,...  \n",
              "3      [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...  \n",
              "4      [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...  \n",
              "...                                                  ...  \n",
              "11638  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...  \n",
              "11639  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...  \n",
              "11640  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...  \n",
              "11641  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...  \n",
              "11642  [[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...  \n",
              "\n",
              "[11643 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-144a0ea8-aecb-4cbf-9d91-d8a1573df06c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>product_id</th>\n",
              "      <th>product_description</th>\n",
              "      <th>category</th>\n",
              "      <th>img_id</th>\n",
              "      <th>img_array</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>243809c0-9cfc-4486-ad12-3b7a16605ba9</td>\n",
              "      <td>Mirror wall art. Posted by Nisha in Dining, Li...</td>\n",
              "      <td>Home &amp; Garden</td>\n",
              "      <td>64aa79f3-e9fa-417c-a332-714b8ce933f1</td>\n",
              "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1c58d3f9-8b93-47ea-9415-204fcc2a22e6</td>\n",
              "      <td>Morphy Richard’s (model no 48755)Stainless ste...</td>\n",
              "      <td>Home &amp; Garden</td>\n",
              "      <td>4e670f9e-7feb-458f-b529-ac52547abe2b</td>\n",
              "      <td>[[[255, 255, 255], [255, 255, 255], [255, 255,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1c58d3f9-8b93-47ea-9415-204fcc2a22e6</td>\n",
              "      <td>Morphy Richard’s (model no 48755)Stainless ste...</td>\n",
              "      <td>Home &amp; Garden</td>\n",
              "      <td>a864ee52-d91e-46e7-94d1-2418e9bb2877</td>\n",
              "      <td>[[[255, 255, 255], [255, 255, 255], [255, 255,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>860673f1-57f6-47ba-8d2f-13f9e05b8f9a</td>\n",
              "      <td>I have 2 of these - collection only as I don’t...</td>\n",
              "      <td>Home &amp; Garden</td>\n",
              "      <td>bfe77c38-c9eb-47fb-b3d6-31ffdefb6ff9</td>\n",
              "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>59948726-29be-4b35-ade5-bb2fd7331856</td>\n",
              "      <td>Great reclaimed army ammunition box used as co...</td>\n",
              "      <td>Home &amp; Garden</td>\n",
              "      <td>a92e56b7-94fc-41b4-ba6c-f2f224f42bb8</td>\n",
              "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11638</th>\n",
              "      <td>2b0a652b-46a2-4297-b619-5efeeb222787</td>\n",
              "      <td>Pick up only £250Comes with two pistols stocks...</td>\n",
              "      <td>Video Games &amp; Consoles</td>\n",
              "      <td>b1466df4-594b-4f83-a66c-f9835d982c92</td>\n",
              "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11639</th>\n",
              "      <td>719fd40a-870e-4144-b324-55dff2e66fb4</td>\n",
              "      <td>Bought at christmas from currys retailing at £...</td>\n",
              "      <td>Video Games &amp; Consoles</td>\n",
              "      <td>c8488028-bf07-4258-a4c2-56d2fe387835</td>\n",
              "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11640</th>\n",
              "      <td>719fd40a-870e-4144-b324-55dff2e66fb4</td>\n",
              "      <td>Bought at christmas from currys retailing at £...</td>\n",
              "      <td>Video Games &amp; Consoles</td>\n",
              "      <td>dc99e40f-6b15-494d-9fb7-f0d02e9781f9</td>\n",
              "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11641</th>\n",
              "      <td>86d1806b-5575-4a7e-9160-f24f12be6c95</td>\n",
              "      <td>Nintendo Switch console only used 2/3 times. A...</td>\n",
              "      <td>Video Games &amp; Consoles</td>\n",
              "      <td>ed2aaf88-616c-4e6b-af49-bbdf7c0fbd06</td>\n",
              "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11642</th>\n",
              "      <td>86d1806b-5575-4a7e-9160-f24f12be6c95</td>\n",
              "      <td>Nintendo Switch console only used 2/3 times. A...</td>\n",
              "      <td>Video Games &amp; Consoles</td>\n",
              "      <td>c6113145-89c8-47cd-9211-38f29d016cc7</td>\n",
              "      <td>[[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11643 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-144a0ea8-aecb-4cbf-9d91-d8a1573df06c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-144a0ea8-aecb-4cbf-9d91-d8a1573df06c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-144a0ea8-aecb-4cbf-9d91-d8a1573df06c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import time\n",
        "import copy\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.functional as F\n",
        "from google.colab import drive\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertModel\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f'Using {device} for inference')\n",
        "\n",
        "products = np.load(\"data/text_model_data.npy\", allow_pickle=True)\n",
        "products = pd.DataFrame(products, columns=['product_id', 'product_description', 'category', 'img_id', 'img_array'])\n",
        "products"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDOAMB1va6oK"
      },
      "source": [
        "Create text dataset made up of tokenised text and numerical labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2oepLgoAvWOh"
      },
      "outputs": [],
      "source": [
        "class ImageTextDataset(Dataset):\n",
        "    def __init__(self, data, text_pad_length=None, transform=None):\n",
        "        super().__init__()\n",
        "        self.labels = data['category'].to_list()\n",
        "        self.num_classes = len(set(self.labels))\n",
        "        self.descriptions = data['product_description']\n",
        "        self.descriptions_list = self.descriptions.to_list()\n",
        "        if text_pad_length is None:\n",
        "          avg_description_length = sum( map(len, self.descriptions_list) ) / len(self.descriptions_list)\n",
        "          self.pad_length = round(avg_description_length)\n",
        "        else:\n",
        "          self.pad_length = text_pad_length\n",
        "        self.img_array = data['img_array']\n",
        "        self.files = data['img_id']\n",
        "        self.category_encoder = {y: x for (x, y) in enumerate(set(self.labels))}\n",
        "        self.category_decoder = {x: y for (x, y) in enumerate(set(self.labels))}\n",
        "        self.tokeniser = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
        "        self.transform = transform\n",
        "        if transform is None:\n",
        "          self.transform = transforms.Compose([\n",
        "                                               transforms.RandomHorizontalFlip(p=0.3),\n",
        "                                               transforms.ToTensor(),\n",
        "                                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "          ])\n",
        "                                               \n",
        "        assert len(self.descriptions_list) == len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = self.category_encoder[self.labels[index]]\n",
        "        label = torch.as_tensor(label).to(device)\n",
        "        image = Image.fromarray(self.img_array[index])\n",
        "        image = self.transform(image).to(device)\n",
        "        sentence = self.descriptions.iloc[index]\n",
        "        encoded = self.tokeniser.batch_encode_plus([sentence], max_length=self.pad_length, padding='max_length', truncation=True)\n",
        "        encoded = {key: torch.LongTensor(value) for key, value in encoded.items()}\n",
        "        with torch.no_grad():\n",
        "          description = self.model(**encoded).last_hidden_state.swapaxes(1,2)\n",
        "\n",
        "        description = description.squeeze(0).to(device)\n",
        "\n",
        "        return (image, description, label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.descriptions_list)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDmyfpYebly0"
      },
      "source": [
        "Can you do data augmentation with NLP?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "04C08ar30O2W"
      },
      "outputs": [],
      "source": [
        "class BertCNN(torch.nn.Module):\n",
        "    def __init__(self, embedding_size=768, out_size=13):\n",
        "        super(BertCNN, self).__init__()\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            nn.Conv1d(embedding_size, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "            nn.Conv1d(256, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "            nn.Conv1d(128, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "            nn.Conv1d(64, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(141 , 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(141, out_size)\n",
        "            )\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = self.layers(X)\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedModel(nn.Module):\n",
        "  def __init__(self, embedding_size=768, out_size=13):\n",
        "    super(CombinedModel, self).__init__()\n",
        "    resnet50 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)\n",
        "    out_features = resnet50.fc.out_features\n",
        "    self.image_classifier = nn.Sequential(resnet50, nn.Linear(out_features, 128)).to(device)\n",
        "    self.text_classifier = BertCNN()\n",
        "    self.main = nn.Sequential(nn.Linear(256, out_size))\n",
        "\n",
        "  def forward(self, image_features, text_features):\n",
        "    image_features = self.image_classifier(image_features)\n",
        "    text_features = self.text_classifier(text_features)\n",
        "    combined_features = torch.cat((image_features, text_features), 1)\n",
        "    combined_features = self.main(combined_features)\n",
        "    return combined_features"
      ],
      "metadata": {
        "id": "QwLoghK4JU4G"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fdHuuCmO1eMR"
      },
      "outputs": [],
      "source": [
        "def train_validation_test_split(data, train_percent, val_percent, batch_size, shuffle=True):\n",
        "    data_size = len(data)\n",
        "    train_size = int(train_percent * data_size)\n",
        "    val_size = int(val_percent * data_size)\n",
        "    test_size = data_size - (train_size + val_size)\n",
        "\n",
        "    indices = list(range(data_size))\n",
        "    if shuffle:\n",
        "      np.random.shuffle(indices)\n",
        "\n",
        "    train_split_indice = indices[:train_size]\n",
        "    val_split_indice = indices[train_size:(val_size + train_size)]\n",
        "    test_split_indice =indices[(val_size + train_size):data_size]\n",
        "\n",
        "    # Create dictionary of dataset sizes\n",
        "    dataset_sizes = {\"train\": train_size,\n",
        "    \"val\": val_size,\n",
        "    \"test\": test_size\n",
        "    }\n",
        "\n",
        "    splits = {\"train\": train_split_indice,\n",
        "    \"val\": val_split_indice,\n",
        "    \"test\": test_split_indice\n",
        "    }\n",
        "\n",
        "    samplers = {\"train\": SubsetRandomSampler(splits[\"train\"]),\n",
        "        \"val\": SubsetRandomSampler(splits[\"val\"]),\n",
        "        \"test\": SubsetRandomSampler(splits[\"test\"])\n",
        "        }\n",
        "    \n",
        "    dataset = {phase: data\n",
        "        for phase in [\"train\", \"val\", \"test\"]}\n",
        "\n",
        "    data_loader = {phase: DataLoader(dataset[phase], batch_size=batch_size, sampler=samplers[phase])\n",
        "        for phase in [\"train\", \"val\", \"test\"]}\n",
        "\n",
        "    return data_loader, dataset_sizes\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ImageTextDataset(products, text_pad_length=292, transform=None)\n",
        "batch_size = 64\n",
        "data_loader, dataset_sizes = train_validation_test_split(dataset, 0.8, 0.2, 32)\n",
        "print(dataset_sizes)\n",
        "combined = CombinedModel()\n",
        "combined.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimiser = optim.Adam(combined.parameters(), lr = 0.001)\n",
        "epochs = 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-FVfBgWMWXa",
        "outputId": "97407af7-d2c0-45cb-9cc8-28e2c1cf8ad1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Using cache found in /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': 9314, 'val': 2328, 'test': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and save model weights for implementation into API"
      ],
      "metadata": {
        "id": "jiZBSgQ-v4lU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    since = time.time()\n",
        "    hist_accuracy = []\n",
        "    hist_loss = []\n",
        "    accuracy = 0\n",
        "    pbar = tqdm(enumerate(data_loader['train']), total=len(data_loader['train']))\n",
        "    for i, batch in pbar:\n",
        "        image_features, text_features, labels = batch\n",
        "        image_features = image_features.to(device)\n",
        "        text_features = text_features.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimiser.zero_grad()\n",
        "        outputs = combined(image_features, text_features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        accuracy = torch.mean((torch.argmax(outputs, dim=1) == labels).float().item())\n",
        "        hist_accuracy.append(accuracy)\n",
        "        hist_loss.append(loss.item())\n",
        "        optimiser.step()\n",
        "        pbar.set_description(f\"Epoch = {epoch+1}/{epochs}. loss = {loss.item():.4f} Acc = {round(torch.sum(torch.argmax(outputs, dim=1) == labels).item()/len(labels), 2)}, Total_acc = {round(np.mean(hist_accuracy), 2)}\" )\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "print(time_elapsed)\n",
        "\n",
        "torch.save(combined.state_dict(), 'combined_model.pt')\n",
        "\n",
        "with open('combined_decoder.pkl', 'wb') as f:\n",
        "    pickle.dump(dataset.category_decoder, f)"
      ],
      "metadata": {
        "id": "TAoNsOarLXCr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "product_classification_mixed_modal.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "8ad305deca811d2df6c57b148dec1e835d783e41823b85ed7281e336135b66d8"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('fb_marketplace')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}