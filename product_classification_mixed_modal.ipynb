{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdamWatson91/fb_marketplace/blob/main/product_classification_mixed_modal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoJiSyiPA9z6",
        "outputId": "bfb5e779-d9cb-41c8-d39a-8511fa7fdd80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "jpLZMPyv4N5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebb407f2-f766-4cd3-bfe3-027b17389e85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir data\n",
        "!cp \"/content/drive/MyDrive/text_model_data.npy\" \"data/text_model_data.npy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqBcnBeBv_R3",
        "outputId": "004947dc-5055-46ee-8a41-b4525afdff89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ],
      "source": [
        "# Install module for Bert model\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Module imports and dataset imports"
      ],
      "metadata": {
        "id": "ux_rXS4FtNU3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "7dIA53ML1eMN",
        "outputId": "0fec8098-b80a-4f3a-a4db-1187852a8725"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda for inference\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-105-8b99f74be470>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Using {device} for inference'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mproducts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/text_model_data.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mproducts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproducts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'product_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'product_description'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'category'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'img_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'img_array'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mproducts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 441\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    442\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    746\u001b[0m             \u001b[0mpickle_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0;31m# Friendlier error message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: pickle data was truncated"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import time\n",
        "import copy\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.functional as F\n",
        "from google.colab import drive\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertModel\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "# %reload_ext tensorboard\n",
        "# %tensorboard --logdir runs\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f'Using {device} for inference')\n",
        "\n",
        "products = np.load(\"data/text_model_data.npy\", allow_pickle=True)\n",
        "products = pd.DataFrame(products, columns=['product_id', 'product_description', 'category', 'img_id', 'img_array'])\n",
        "products"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDOAMB1va6oK"
      },
      "source": [
        "Create text dataset made up of tokenised text and numerical labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oepLgoAvWOh"
      },
      "outputs": [],
      "source": [
        "class ImageTextDataset(Dataset):\n",
        "    def __init__(self, data, text_pad_length=None, transform=None):\n",
        "        super().__init__()\n",
        "        self.labels = data['category'].to_list()\n",
        "        self.num_classes = len(set(self.labels))\n",
        "        self.descriptions = data['product_description']\n",
        "        self.descriptions_list = self.descriptions.to_list()\n",
        "        # self.max_length_description = max(self.descriptions, key=len)\n",
        "        if text_pad_length is None:\n",
        "          avg_description_length = sum( map(len, self.descriptions_list) ) / len(self.descriptions_list)\n",
        "          self.pad_length = round(avg_description_length)\n",
        "        else:\n",
        "          self.pad_length = text_pad_length\n",
        "        self.img_array = data['img_array']\n",
        "        self.files = data['img_id']\n",
        "        # NEED TO SAVE THESE\n",
        "        self.category_encoder = {y: x for (x, y) in enumerate(set(self.labels))}\n",
        "        self.category_decoder = {x: y for (x, y) in enumerate(set(self.labels))}\n",
        "        # self.vocab = self.create_vocab(self.descriptions_list)\n",
        "        # self.vocab_size = len(self.vocab)\n",
        "        self.tokeniser = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
        "        self.transform = transform\n",
        "        if transform is None:\n",
        "          self.transform = transforms.Compose([\n",
        "                                               transforms.RandomHorizontalFlip(p=0.3),\n",
        "                                               transforms.ToTensor(),\n",
        "                                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "          ])\n",
        "                                               \n",
        "        assert len(self.descriptions_list) == len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = self.category_encoder[self.labels[index]]\n",
        "        label = torch.as_tensor(label).to(device)\n",
        "        image = Image.fromarray(self.img_array[index])\n",
        "        image = self.transform(image).to(device)\n",
        "        sentence = self.descriptions.iloc[index]\n",
        "        encoded = self.tokeniser.batch_encode_plus([sentence], max_length=self.pad_length, padding='max_length', truncation=True)\n",
        "        encoded = {key: torch.LongTensor(value) for key, value in encoded.items()}\n",
        "        with torch.no_grad():\n",
        "          description = self.model(**encoded).last_hidden_state.swapaxes(1,2)\n",
        "\n",
        "        description = description.squeeze(0).to(device)\n",
        "\n",
        "        return (image, description, label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.descriptions_list)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDmyfpYebly0"
      },
      "source": [
        "Can you do data augmentation with NLP?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04C08ar30O2W"
      },
      "outputs": [],
      "source": [
        "class BertCNN(torch.nn.Module):\n",
        "    def __init__(self, embedding_size=768, out_size=13):\n",
        "        super(BertCNN, self).__init__()\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            nn.Conv1d(embedding_size, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "            nn.Conv1d(256, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "            nn.Conv1d(128, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "            nn.Conv1d(64, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(141 , 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(141, out_size)\n",
        "            )\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = self.layers(X)\n",
        "        return X\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedModel(nn.Module):\n",
        "  def __init__(self, embedding_size=768, out_size=13):\n",
        "    super(CombinedModel, self).__init__()\n",
        "    resnet50 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)\n",
        "    out_features = resnet50.fc.out_features\n",
        "    self.image_classifier = nn.Sequential(resnet50, nn.Linear(out_features, 128)).to(device)\n",
        "    self.text_classifier = BertCNN()\n",
        "    self.main = nn.Sequential(nn.Linear(256, out_size))\n",
        "\n",
        "  def forward(self, image_features, text_features):\n",
        "    image_features = self.image_classifier(image_features)\n",
        "    text_features = self.text_classifier(text_features)\n",
        "    combined_features = torch.cat((image_features, text_features), 1)\n",
        "    combined_features = self.main(combined_features)\n",
        "    return combined_features"
      ],
      "metadata": {
        "id": "QwLoghK4JU4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ImageTextDataset(products, text_pad_length=292, transform=None)\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "combined = CombinedModel()\n",
        "combined.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimiser = optim.Adam(combined.parameters(), lr = 0.001)\n",
        "epochs = 5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-FVfBgWMWXa",
        "outputId": "09beda81-e53a-4d85-ecb0-93b0be766ba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Using cache found in /root/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    since = time.time()\n",
        "    hist_accuracy = []\n",
        "    hist_loss = []\n",
        "    accuracy = 0\n",
        "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "    for i, batch in pbar:\n",
        "        image_features, text_features, labels = batch\n",
        "        image_features = image_features.to(device)\n",
        "        text_features = text_features.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimiser.zero_grad()\n",
        "        outputs = combined(image_features, text_features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        accuracy = torch.mean((torch.argmax(outputs, dim=1) == labels).float().item())\n",
        "        hist_accuracy.append(accuracy)\n",
        "        hist_loss.append(loss.item())\n",
        "        optimiser.step()\n",
        "        pbar.set_description(f\"Epoch = {epoch+1}/{epochs}. loss = {loss.item():.4f} Acc = {round(torch.sum(torch.argmax(outputs, dim=1) == labels).item()/len(labels), 2)}, Total_acc = {round(np.mean(hist_accuracy), 2)}\" )\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "print(time_elapsed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "TAoNsOarLXCr",
        "outputId": "9696283e-7ab4-4a37-d05d-d7bb4d1b7068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/364 [00:19<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-393edfca9b8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-98-16a2ce44f94d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image_features, text_features)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtext_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcombined_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mcombined_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcombined_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x141 and 256x13)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uddP7xzVuvuq"
      },
      "outputs": [],
      "source": [
        "def train_model(model, num_epochs, optimiser, criterion, scheduler=None, model_load_location=None):\n",
        "    writer = SummaryWriter()\n",
        "    \n",
        "    since = time.time()\n",
        "\n",
        "    # keep best model at every epoch by copying weights\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # print current epoch\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        # pbar = tqdm(enumerate(['train', 'val']), total=len(['train', 'val']))\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                print(\"Training...\")\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                print(\"Validating...\")\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for batch_idx, batch in enumerate(data_loader[phase]):\n",
        "                print(batch_idx)\n",
        "                features, labels = batch\n",
        "                features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimiser.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(features)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    # loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimiser.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * features.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            if phase == 'train' and scheduler != None:\n",
        "                scheduler.step()\n",
        "            print(dataset_sizes[phase])\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
        "\n",
        "            writer.add_scalar('Epoch Loss', epoch_loss, epoch)\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            if phase == \"train\":\n",
        "                writer.add_scalar(\"Training Accuracy\", epoch_acc*100, epoch)\n",
        "            else:\n",
        "                writer.add_scalar(\"Validation Accuracy\", epoch_acc*100, epoch)\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            writer.flush()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    # if model_save_location != None:\n",
        "    #     torch.save(model.state_dict(), model_save_location)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdHuuCmO1eMR",
        "outputId": "3bb48794-3f1b-4bc0-ded0-456701d04f9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': 4570, 'val': 1305, 'test': 654}\n",
            "tensor([[[-0.5891,  0.3420, -0.3100,  ..., -0.2536, -0.1784, -0.3610],\n",
            "         [-0.1581, -0.3863, -0.6790,  ..., -0.6691, -0.6779, -0.8079],\n",
            "         [ 0.7513,  1.1665,  0.5932,  ...,  0.5469,  0.7161,  0.5418],\n",
            "         ...,\n",
            "         [-0.3239,  0.4387, -0.0340,  ...,  0.2407,  0.3058,  0.3273],\n",
            "         [ 0.1625,  0.6132,  0.0532,  ...,  0.3102,  0.3489,  0.3322],\n",
            "         [ 0.2730, -0.3002,  0.1331,  ..., -0.4400, -0.3763, -0.4696]],\n",
            "\n",
            "        [[-0.9658,  0.0211, -0.2694,  ...,  0.0244, -0.2702, -0.2160],\n",
            "         [-0.0404,  0.3615,  0.4668,  ..., -0.0359, -0.3199, -0.2416],\n",
            "         [ 0.3220,  0.4451,  0.4582,  ...,  0.3325,  0.3348,  0.4227],\n",
            "         ...,\n",
            "         [ 0.0284,  0.2936,  0.1670,  ..., -0.1714, -0.0246,  0.0571],\n",
            "         [-0.0577,  1.0571,  0.1832,  ..., -0.1434, -0.3309, -0.1816],\n",
            "         [ 0.2646,  0.0517, -1.8421,  ..., -0.2691, -0.3191, -0.4349]],\n",
            "\n",
            "        [[-0.8197,  0.4421,  0.1649,  ..., -0.0096, -0.1730, -0.1272],\n",
            "         [-0.1010,  0.2714,  0.5013,  ..., -0.3048, -0.2682, -0.2546],\n",
            "         [ 0.3962,  0.3076,  0.4311,  ...,  0.5866,  0.4180,  0.5013],\n",
            "         ...,\n",
            "         [-0.1117,  0.1914,  0.2008,  ..., -0.1294, -0.1827, -0.1377],\n",
            "         [ 0.2627,  0.1177, -0.5022,  ..., -0.3604, -0.3986, -0.4061],\n",
            "         [ 0.5034, -0.0966, -0.2887,  ..., -0.0623, -0.0248, -0.0896]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.1857, -0.0286,  0.2456,  ..., -0.0603,  0.0697,  0.4823],\n",
            "         [ 0.2083, -0.1960,  0.1845,  ...,  0.0620,  0.0939,  0.1524],\n",
            "         [ 0.2363,  0.1488,  0.0647,  ...,  0.8256,  0.8840,  0.8042],\n",
            "         ...,\n",
            "         [-0.4680, -0.6057, -0.4386,  ..., -0.3188, -0.2968, -0.0522],\n",
            "         [ 0.2220,  1.3576,  0.3635,  ..., -0.0414, -0.0112,  0.1719],\n",
            "         [ 0.4013, -0.1823,  0.0198,  ..., -0.0807,  0.0019, -0.1581]],\n",
            "\n",
            "        [[-0.9533,  0.0213, -0.1640,  ..., -0.2247, -0.3265, -0.3108],\n",
            "         [-0.2265, -0.1942, -0.4315,  ..., -0.8818, -0.9112, -0.9599],\n",
            "         [ 0.0681,  0.4002,  0.4000,  ...,  0.0366, -0.0028, -0.0322],\n",
            "         ...,\n",
            "         [-0.2298, -0.4315,  0.1529,  ...,  0.1319,  0.1612,  0.1843],\n",
            "         [-0.1710,  0.0043, -0.0688,  ...,  0.1371,  0.1394,  0.1895],\n",
            "         [ 0.3863, -0.2478, -0.6125,  ..., -0.2889, -0.3307, -0.3391]],\n",
            "\n",
            "        [[-0.6426, -0.5302, -0.6915,  ..., -0.1994,  0.1630, -0.4196],\n",
            "         [-0.0987, -0.4743, -0.0672,  ..., -0.4707, -0.2840, -0.6340],\n",
            "         [ 0.3928,  0.5491,  0.7989,  ...,  0.3607,  0.4819,  0.4311],\n",
            "         ...,\n",
            "         [-0.4543,  0.0437,  0.3117,  ...,  0.1567, -0.0644,  0.2071],\n",
            "         [ 0.1910, -0.0559, -0.5356,  ...,  0.0761,  0.0107,  0.2965],\n",
            "         [ 0.2093,  0.1876, -0.2058,  ..., -0.3230, -0.2300, -0.3466]]])\n",
            "tensor([ 2,  8,  6,  8,  8,  6,  4, 12,  2, 10,  8, 12, 10,  0,  8,  7,  3,  8,\n",
            "        10, 10,  8, 10,  0,  6,  7,  6,  6,  7,  1,  2,  7, 12],\n",
            "       device='cuda:0')\n",
            "torch.Size([32, 768, 292])\n",
            "{'train': 4570, 'val': 1305, 'test': 654}\n"
          ]
        }
      ],
      "source": [
        "def train_validation_test_split(data, train_percent, val_percent, batch_size, shuffle=True):\n",
        "    data_size = len(data)\n",
        "    train_size = int(train_percent * data_size)\n",
        "    val_size = int(val_percent * data_size)\n",
        "    test_size = data_size - (train_size + val_size)\n",
        "\n",
        "    indices = list(range(data_size))\n",
        "    if shuffle:\n",
        "      np.random.shuffle(indices)\n",
        "\n",
        "    train_split_indice = indices[:train_size]\n",
        "    val_split_indice = indices[train_size:(val_size + train_size)]\n",
        "    test_split_indice =indices[(val_size + train_size):data_size]\n",
        "\n",
        "    # Create dictionary of dataset sizes\n",
        "    dataset_sizes = {\"train\": train_size,\n",
        "    \"val\": val_size,\n",
        "    \"test\": test_size\n",
        "    }\n",
        "\n",
        "    splits = {\"train\": train_split_indice,\n",
        "    \"val\": val_split_indice,\n",
        "    \"test\": test_split_indice\n",
        "    }\n",
        "\n",
        "    samplers = {\"train\": SubsetRandomSampler(splits[\"train\"]),\n",
        "        \"val\": SubsetRandomSampler(splits[\"val\"]),\n",
        "        \"test\": SubsetRandomSampler(splits[\"test\"])\n",
        "        }\n",
        "    \n",
        "    dataset = {phase: data\n",
        "        for phase in [\"train\", \"val\", \"test\"]}\n",
        "\n",
        "    data_loader = {phase: DataLoader(dataset[phase], batch_size=batch_size, sampler=samplers[phase])\n",
        "        for phase in [\"train\", \"val\", \"test\"]}\n",
        "\n",
        "    return data_loader, dataset_sizes\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "product_classification_mixed_modal.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "8ad305deca811d2df6c57b148dec1e835d783e41823b85ed7281e336135b66d8"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('fb_marketplace')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}